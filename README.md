# Adversarial_Attack_and_Mitigation_Summary


Adversarial Attack and Mitigation on CIFAR-10

This project explores adversarial attacks on deep learning models and implements simple mitigation strategies. It focuses on the Fast Gradient Sign Method (FGSM) attack applied to a CNN trained on the CIFAR-10 dataset and evaluates the impact of techniques like JPEG compression and Gaussian blurring for defense.
üì¶ Dataset

    CIFAR-10: 60,000 32√ó32 RGB images across 10 classes:

        Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse, Ship, Truck

    Split: 50,000 training images + 10,000 test images

    Preprocessing:

        Converted to PyTorch tensors

        Normalized to [0, 1] pixel range

        Data augmentation used during mitigation (Gaussian blur, JPEG compression)

üèóÔ∏è Model

    SimpleCNN architecture:

        2 convolutional blocks (Conv2D + ReLU + MaxPool)

        Fully connected layer (Linear(64*8*8 ‚Üí 10))

    Training:

        1 epoch

        Adam optimizer (learning rate: 0.001)

    Output:

        Converged training loss ~1.53

        Model checkpoint saved for attack/mitigation evaluation

‚öîÔ∏è Adversarial Attack

    Method: Fast Gradient Sign Method (FGSM)

    Objective: Fool the classifier by generating perturbed images

    Results: Significant drop in model accuracy on attacked images

üõ°Ô∏è Mitigation Techniques

- Gaussian Blur: Applies a smoothing filter to reduce high-frequency noise introduced by adversarial perturbations.

- JPEG Compression: Compresses images at a low quality (q=30), helping to discard subtle adversarial noise.

- Median Filtering: Uses a median filter (k=3) to effectively remove salt-and-pepper-like noise introduced by attacks.

- Total Variation Denoising: An optimization-based technique that reduces pixel-level noise while preserving essential structures, helping restore clean-like features in adversarially perturbed images.

